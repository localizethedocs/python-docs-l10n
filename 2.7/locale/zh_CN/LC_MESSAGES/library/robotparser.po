# SOME DESCRIPTIVE TITLE.
# Copyright (C) 1990-2025, Python Software Foundation
# This file is distributed under the same license as the Python package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Python 2.7\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-08-10 15:08+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"Language: zh_CN\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:3
msgid ":mod:`robotparser` ---  Parser for robots.txt"
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:18
msgid ""
"The :mod:`robotparser` module has been renamed :mod:`urllib.robotparser` in "
"Python 3. The :term:`2to3` tool will automatically adapt imports when "
"converting your sources to Python 3."
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:23
msgid ""
"This module provides a single class, :class:`RobotFileParser`, which answers "
"questions about whether or not a particular user agent can fetch a URL on "
"the Web site that published the :file:`robots.txt` file.  For more details "
"on the structure of :file:`robots.txt` files, see http://www.robotstxt.org/"
"orig.html."
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:31
msgid ""
"This class provides methods to read, parse and answer questions about the :"
"file:`robots.txt` file at *url*."
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:37
msgid "Sets the URL referring to a :file:`robots.txt` file."
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:42
msgid "Reads the :file:`robots.txt` URL and feeds it to the parser."
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:47
msgid "Parses the lines argument."
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:52
msgid ""
"Returns ``True`` if the *useragent* is allowed to fetch the *url* according "
"to the rules contained in the parsed :file:`robots.txt` file."
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:59
msgid ""
"Returns the time the ``robots.txt`` file was last fetched.  This is useful "
"for long-running web spiders that need to check for new ``robots.txt`` files "
"periodically."
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:66
msgid ""
"Sets the time the ``robots.txt`` file was last fetched to the current time."
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:69
msgid ""
"The following example demonstrates basic use of the RobotFileParser class. ::"
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:71
msgid ""
">>> import robotparser\n"
">>> rp = robotparser.RobotFileParser()\n"
">>> rp.set_url(\"http://www.musi-cal.com/robots.txt\")\n"
">>> rp.read()\n"
">>> rp.can_fetch(\"*\", \"http://www.musi-cal.com/cgi-bin/search?"
"city=San+Francisco\")\n"
"False\n"
">>> rp.can_fetch(\"*\", \"http://www.musi-cal.com/\")\n"
"True"
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:11
msgid "WWW"
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:11
msgid "World Wide Web"
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:11
msgid "URL"
msgstr ""

#: /home/runner/work/python-docs-l10n/python-docs-l10n/out/repo/Doc/library/robotparser.rst:11
msgid "robots.txt"
msgstr ""
